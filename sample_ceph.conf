## ceph conf file!  # use semi-colon to put a comment!  
[global]
    #auth supported = cephx
    auth cluster required = cephx
    auth service required = cephx
    auth client required = cephx
    max open files =131072
    #auth supported = none
    keyring = /etc/ceph/keyring.admin
    public network = 10.239.149.1/24
    cluster network = 192.168.4.0/24
    osd pool default size = 3
    osd pool default min size = 2
#    debug ms = 1
 #   debug mds = 20
#    debug mds balancer = 20
#    debug mds log = 20
#    debug mds migrator = 20
#[client.glance]
#    keyring = /etc/ceph/keyring.admin
[mds]
    mds data = /var/lib/ceph/mds/$cluster-$id
    keyring = /etc/ceph/keyring.$name
#    debug mds = 20
#    debug paxos = 20
#    debug auth = 20
#    debug mds_migrator = 20
#    debug monc = 20
[mds.1]
    host = shzcephmon01
    devs = /dev/sdb
    #public addr = 10.239.149.9
    #cluster addr = 192.168.4.100

[osd]
    osd data = /var/lib/ceph/osd/$cluster-$id
    #osd journal = /var/lib/ceph/osd/$cluster-$id/journal
    osd journal size = 10240
    osd class dir = /usr/lib/rados-classes
    #osd class dir = $(libdir)/rados-classes
    keyring = /etc/ceph/keyring.$name
    #debug osd = 20
    #debug filestore = 20
    #debug journal = 20
    #debug monc = 20
    osd mkfs type = xfs
    osd mount options xfs = rw,noatime,nodiratime
    osd mkfs options xfs = -f
    # working with ext4
    #filestore xattr use omap = true
    filestore min sync interval = 1.0
    # solve rbd data corruption
    filestore fiemap = true
    public network = 10.239.149.1/24
    cluster network = 192.168.4.0/24
[osd.0]
    host = shzceph01
    osd journal = /dev/sdb
    public addr = 10.239.149.6
    cluster addr = 192.168.4.101
    devs = /dev/sde
#raid 0
[osd.1]
    host = shzceph01
    osd journal = /dev/sdc
    public addr = 10.239.149.6
    cluster addr = 192.168.4.101
    devs = /dev/sdd
[osd.2]	
    host = shzceph02
    osd journal = /dev/sdb
    public addr = 10.239.149.7
    cluster addr = 192.168.4.102
    devs = /dev/sde
#raid 0
[osd.3]	
    host = shzceph02
    osd journal = /dev/sdc
    public addr = 10.239.149.7
    cluster addr = 192.168.4.102
    devs = /dev/sdd
[osd.4]	
    host = shzceph03
    osd journal = /dev/sdb
    public addr = 10.239.149.8
    cluster addr = 192.168.4.103
    devs = /dev/sde
#raid 0
[osd.5]	
    host = shzceph03
    osd journal = /dev/sdc
    public addr = 10.239.149.8
    cluster addr = 192.168.4.103
    devs = /dev/sdd
[mon]
    mon data = /var/lib/ceph/mon/$cluster-$id
#    debug ms = 1
#    debug mon = 20
#    debug paxos = 20
#    debug auth = 20
[mon.a]
    host = shzcephmon01
    mon addr = 10.239.149.9:6789
    devs = /dev/sdc
[client.radosgw.gateway]
    host = shzcephmon01
    keyring = /etc/ceph/keyring.radosgw.gateway
    rgw socket path = /tmp/radosgw.sock
    log file = /var/log/ceph/radosgw.log
    rgw print continue = false
    rgw enable usage log = true
    rgw enable ops log = true
    rgw enable log rados = true
    debug rgw = 20
    debug ms = 1
    #configurations for integration with keystone
    rgw keystone url = http://127.0.0.1:35357
    rgw keystone admin token = 5c1e02653fc121c5ad82
    rgw keystone accepted roles = ceph_admin, ceph_user
    rgw keystone token cache size = 100
    rgw keystone revocation interval = 60
    rgw s3 auth use keystone = true
    nss db path = /var/ceph/nss
[client]
    #debug ms = 1
    #debug client = 10 #20
    rbd cache = true    
    rbd cache writethrough until flush = true
